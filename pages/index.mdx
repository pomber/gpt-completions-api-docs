<CH.Scrollycoding>

# OpenAI Chat Completion Unofficial Docs

[Edit this page](https://pr.new/github.com/pomber/gpt-completions-api-docs/edit/main/pages/index.mdx) |
[GitHub](https://github.com/pomber/gpt-completions-api-docs)

OpenAI's GPT (generative pre-trained transformer) models have been trained to understand natural language and code. GPTs provide text outputs in response to their inputs. The inputs to GPTs are also referred to as "prompts". Designing a prompt is essentially how you “program” a GPT model, usually by providing instructions or some examples of how to successfully complete a task.

Given a list of messages comprising a conversation, the model will return a response.

```js
// from ../snippets/base.js
```

---

## API key

<nav id="api-key" />

```js focus=3
// from ../snippets/base.js
```

The first thing you'll need is the _`OPENAI_API_KEY`_. Visit your [API Keys](https://platform.openai.com/account/api-keys) page to retrieve the API key you'll use in your requests.

**Remember that your API key is a secret!** Do not share it with others or expose it in any client-side code (browsers, apps). Production requests must be routed through your own backend server where your API key can be securely loaded from an environment variable or key management service.

---

## Model

<nav id="model" />

```js focus=8
// from ../snippets/base.js
```

_`model`_ is a required field.

It should be one of:

- `gpt-3.5-turbo`
- `gpt-3.5-turbo-0613`
- `gpt-3.5-turbo-16k`
- `gpt-3.5-turbo-16k-0613`
- `gpt-4`
- `gpt-4-0613`
- `gpt-4-32k`
- `gpt-4-32k-0613`

If you don't know which one to use, start with `gpt-3.5-turbo`.

June 27th, 2023 from gpt-4-0314 to gpt-4-0613

`gpt-3.5-turbo` models are cheaper and faster.

`gpt-4` models have broader general knowledge and advanced reasoning. But they are more expensive, slower, and you need to [ask for access](https://openai.com/waitlist/gpt-4-api) to use them.

---

## Messages

<nav id="messages" />

```js focus=9:12
// from ../snippets/base.js
```

_`messages`_ is a required array. A list of messages comprising the conversation so far.

Each message follows the format:

```js
{
  role: "system" | "user" | "assistant" | "function",
  content: string | undefined,
  name: string | undefined,
  function_call: object | undefined,
}
```

The first message in the array is the prompt. And it's the only message with _`role: "system"`_.

---

## Functions

<nav id="functions" />

```js focus=13:29
// from ../snippets/functions.js
```

An optional list of functions the model may generate JSON inputs for.

The latest models (`gpt-3.5-turbo-0613` and `gpt-4-0613`) have been fine-tuned to both detect when a function should to be called (depending on the input) and to respond with JSON that adheres to the function signature. With this capability also comes potential risks. We strongly recommend building in user confirmation flows before taking actions that impact the world on behalf of users (sending an email, posting something online, making a purchase, etc).

---

## Parameters

<nav id="parameters" />

```js focus=17:26
// from ../snippets/functions.js
```

The parameters the functions accepts, described as a JSON Schema object. See the [JSON Schema reference](https://json-schema.org/understanding-json-schema/) for documentation about the format.

---

## Function Call

<nav id="function-call" />

```js focus=30,50:58
// from ../snippets/functions.js
```

Controls how the model responds to function calls. "none" means the model does not call a function and responds to the end-user. `"auto"` means the model can pick between an end-user or calling a function. Specifying a particular function via _`{"name": "my_function"}`_ forces the model to call that function.

`"none"` is the default when no functions are present. `"auto"` is the default if functions are present.

---

## User

<nav id="user" />

---

## Stream

<nav id="stream" />

---

## Temperature

<nav id="temperature" />

```js focus=17
// from ../snippets/base.js
```

Controls the randomness of the model's output. This parameter is a decimal value typically ranging from _`0`_ to _`1`_. A lower temperature (_`0.1`_ or _`0.2`_) makes the output more deterministic and focused, with the model likely to choose only the most probable word at each step.

A higher temperature (_`0.8`_ or _`1`_) makes the output more diverse and random, taking more chances in its choices of words.

---

## Choices

<nav id="choices" />

</CH.Scrollycoding>
