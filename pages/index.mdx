<CH.Scrollycoding>

# GPT API Unofficial Docs

_This is a work in progress. If you want to contribute, here is the [GitHub repo](https://github.com/pomber/gpt-docs)._

This site offers an alternative take on the [official API reference](https://beta.openai.com/docs/api-reference/chat) and the [official GPT guide](https://platform.openai.com/docs/guides/gpt).

In its most basic form, the Chat Completion API receives a _`system`_ message (the prompt) and a _`user`_ message, and then returns an _`assistant`_ message responding to the user.

```js focus=9:12,17:20 mark=11[30:42],19[14:49]
// from ../snippets/start.js
```

In this example, we are using the [OpenAI node library](https://github.com/openai/openai-node), but the focus of this guide will be the request and response format, which is the same for the [python library](https://github.com/openai/openai-python) and the HTTP API.

---

## Request and Response

```js focus=7[54],8:24,25[1],28[30],29:47,48[1]
// from ../snippets/base.js
```

The goal of this guide is to help you understand all the fields in the request and response. There are a lot of fields. Most are optional, some are required. Some only make sense in combination with others.

From the request, the [_`model`_ and _`messages`_ fields](focus://8:12) are the most important (and also the ones that are required).

From the response, the [_`choices`_ field](focus://33:42) is the most important, that's where you'll find the _`assistant`_'s answer to the last _`user`_ message.

**If you are interested in a particular field, you can click on it to jump to the section that describes it.**

---

## API Key

<nav id="api-key" />

```js focus=3
// from ../snippets/base.js
```

First you'll need the _`OPENAI_API_KEY`_. That means you need to sign up if you haven't already, and then visit your [API keys page](https://platform.openai.com/account/api-keys) to get a key.

**Remember that your API key is a secret!** Do not share it with others or expose it in any client-side code (browsers, apps). Production requests must be routed through your own backend server where your API key can be securely loaded from an environment variable or key management service.

---

## Model

<nav id="model" />

```js focus=8,32
// from ../snippets/base.js
```

The first field from the request is the _`model`_. It's a string with the name of the model to use. If you don't care about the model right now, start with `gpt-3.5-turbo-0613`.

To understand the differences between models, let's deconstruct a model name:

- **gpt-3.5-turbo**-16k-0613

The first part is the model architecture. It could be `gpt-3.5-turbo` or `gpt-4`. `gpt-3.5-turbo` is [cheaper](https://openai.com/pricing) and faster. `gpt-4` is more powerful and it's not available by default, you need to [ask for access](https://openai.com/waitlist/gpt-4-api) to use it.

- gpt-3.5-turbo-**16k**-0613

The next part is the context length in tokens. Tokens can be thought of as pieces of words ([learn more](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them)). A rule of thumb is 100 tokens is about 75 words in english. 16k is the total amount of tokens that the model support per request, not only in the input (`messages` and `functions`), it also includes the output (`message`).

The four options are:

| model             |       context |
| ----------------- | ------------: |
| gpt-3.5-turbo     |  4,096 tokens |
| gpt-3.5-turbo-16k | 16,384 tokens |
| gpt-4             |  8,192 tokens |
| gpt-4-32k         | 32,768 tokens |

Models with larger context are more expensive.

- gpt-3.5-turbo-16k-**0613**

The last part of the model name is the snapshot date.

For example, we have:

- `gpt-3.5-turbo`
- `gpt-3.5-turbo-0301`
- `gpt-3.5-turbo-0613`

Until June 27th, `gpt-3.5-turbo` will be the same as `gpt-3.5-turbo-0301`. After June 27th, `gpt-3.5-turbo` will point to `gpt-3.5-turbo-0613`.
In September 13th, `gpt-3.5-turbo-0301` will be depreacated.

The same upgrade and deprecation cycle will happen to the `gpt-3.5-turbo-0613` snapshot eventually. You can read more about [continuous model upgrades](https://platform.openai.com/docs/models/continuous-model-upgrades) and [model deprecation](https://platform.openai.com/docs/deprecations).

In my opinion, it's better to use the snapshot date in the model name, so you can control when to upgrade to a new snapshot.

These are the latest snapshots available today:

- `gpt-3.5-turbo-0613`
- `gpt-3.5-turbo-16k-0613`
- `gpt-4-0613`
- `gpt-4-32k-0613`

---

## Messages

<nav id="messages" />

```js focus=9:12
// from ../snippets/base.js
```

This is the real body of your request.

To understand _`messages`_, let's see the possible _`role`_ values:

- _`"system"`_ is the role for the first message, the prompt. It's the only message with this role. It's used by the developer to set the style, tone, format, etc. of the response.
- _`"user"`_ these are the messages from the end user. Usually, after the `system` message, the rest of the messages alternate between `user` and `assistant`, ending with a `user` message.
- _`"assistant"`_ these are the messages from the model. But you can also use them to give the model examples of desired behavior.
- _`"function"`_ we'll talk about this [later](#function-role).

---

## Assistant

<nav id="assistant" />

```js focus=9:14,26:29 mark=12[13:23]
// from ../snippets/assistant.js
```

The model doesn't have memory, so after you receive the response, you should append the _`assistant`_ message to the _`messages`_ array before sending the next _`user`_ message.

The other usage of the _`assistant`_ message is to give the model examples of the output you want. You provide a couple of _`user`_ messages and the corresponding _`assistant`_ messages, before sending the real _`user`_ message.

```js
messages: [
  { role: "system", content: "You are a comedian" },
  { role: "user", content: "A Forrest Gump pun" },
  {
    role: "assistant",
    content: "What's Forrest Gump's password? 1forrest1",
  },
  { role: "user", content: "A calendar pun" },
  {
    role: "assistant",
    content: "Can February March? No, but April May",
  },
  { role: "user", content: "A pun about AI" },
]
```

---

## Functions

<nav id="functions" />

```js focus=11,13:26 mark=11[30:51],15[13:28]
// from ../snippets/functions.js
```

GPT wasn't very good at generating data in a predefined format. So the latest models come with an new (optional) input called _`functions`_.

You can pass a list of functions, each with a _`name`_, _`description`_, and _`parameters`_ (as a [JSON Schema](https://json-schema.org/understanding-json-schema/) object).

In the example, we are passing the schema of a _`getCityWeather`_ function. The typescript signature of the function would be something like:

{/* prettier-ignore */}
```ts
function getCityWeather(params: {
  city: string, 
  unit?: "C" | "F",
}) 
```

---

## Function Call

<nav id="function-call" />

```js focus=27,39:47 mark=42[9:21]
// from ../snippets/functions.js
```

The model is trained to detect when to reply with content as usual and when to reply with a function call.

When it chooses to reply with a function call, **the response will have a message from the _`assistant`_ with the _`function_call`_ property instead of the usual _`content`_**. _(Also, the `finish_reason` should be `"function_call"`, but in my tests it sometimes comes as `"stop"`.)_

The _`function_call`_ in the response message is an object with two properties:

- the _`name`_ of the function that the model decided to call
- the _`arguments`_ for that function, which is a stringified JSON, so you need to `JSON.parse` it before using it (be aware that the JSON could be invalid or may not adhere to the schema)

If you want to **force the model to call a particular function**, you can pass the name of the function to the [_`function_call`_ field in the request](focus://27) (not to be confused with the `function_call` field in the response message). You need to pass it inside an object like this:

{/* prettier-ignore */}
```js
function_call: { name: "getCityWeather" }
```

The other two options for the `function_call` field are:

- _`function_call: "auto"`_ which is the default behavior, the model will decide if it should call a function or not
- _`function_call: "none"`_ which forces the model to reply with content as usual

---

## Function Role

<nav id="function-role" />

```js focus=12:24,37:40 mark=21[7:22],23[16:21],39[18:56]
// from ../snippets/function-role.js
```

Once you have the `function_call` provided by the model, you can **run the function** (be careful here, this may be dangerous depending on the function you are calling) and **append the result** to the _`messages`_ array (after the [_`assistant`_ message](focus://12:19)).

You need to set the _`role`_ set to _`function`_, the _`name`_ set to the function name, and the _`content`_ set to the result of the function.

Now that the model has seen the result of the function, it will be able to use it to generate the next response.

---

## Structured Data

<nav id="structured-data" />

```js focus=11,13:35
// from ../snippets/structured-data.js
```

_`functions`_ don't have to be real functions, you can use them to turn any text into structured data.

For example, if you want to extract the named entities from a text, you can make up a `showNamedEntities` function:

```ts
function showNamedEntities(params: {
  entities: { name: string; type: string }[]
})
```

Then if you pass a message like _`{ role: "user", content: "Messi leaving PSG" }`_, the model will reply with a call to the `showNamedEntities` function:

```js
{
  role: "assistant",
  content: null,
  function_call: {
    name: "showNamedEntities",
    arguments: `{
      entities: [
        { name: "Messi", type: "PERSON" },
        { name: "PSG", type: "ORG" },
      ],
    }`,
  },
}
```

And now you can parse and use that data in your app without telling the model that there is no real function called _`showNamedEntities`_.

---

## User

<nav id="user" />

```js focus=11,15
// from ../snippets/base.js
```

You can use the _`user`_ field to pass an ID of the end-user that authored the message.

If OpenAI detects any abuse in your requests [they'll send you](https://platform.openai.com/docs/guides/safety-best-practices) the problematic _`user`_ as part of the notice. As far as I know, that's the only use for this field.

---

## Stream

<nav id="stream" />

```js focus=14,17:24
// from ../snippets/stream.js
```

With _`stream: true`_ we can start showing the response while it's still being generated, without having to wait for the whole response.

OpenAI uses [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format) for the streaming. How you process the stream depends on your tech stack (this example is using Node.js). But the idea is the same, you receive a stream of chunks.

Chunks are strings that starts with `data:` followed by an object. The first chunk looks like this: <span className="break-all"> _`'data: {"id":"chatcmpl-xxxx","object":"chat.completion.chunk","created":1688198627,"model":"gpt-3.5-turbo-0613","choices":[{"index":0,"delta":{"role":"assistant","content":""},"finish_reason":null}]}'`_ </span>

Here you can see all the chunks from the example formatted for readability:

<CH.Slideshow>

```js
data: {
  "id": "chatcmpl-xxxx",
  "object": "chat.completion.chunk",
  "created": 1688198627,
  "model": "gpt-3.5-turbo-0613",
  "choices":[{
    "index": 0,
    "delta": {"role": "assistant", "content": ""},
    "finish_reason": null
  }]
}
```

---

```js focus=8 mark=8[26:32]
data: {
  "id": "chatcmpl-xxxx",
  "object": "chat.completion.chunk",
  "created": 1688198627,
  "model": "gpt-3.5-turbo-0613",
  "choices":[{
    "index": 0,
    "delta": {"content": "Hello"},
    "finish_reason": null
  }]
}
```

---

```js focus=8 mark=8[26:28]
data: {
  "id": "chatcmpl-xxxx",
  "object": "chat.completion.chunk",
  "created": 1688198627,
  "model": "gpt-3.5-turbo-0613",
  "choices":[{
    "index": 0,
    "delta": {"content": "!"},
    "finish_reason": null
  }]
}
```

---

```js focus=8 mark=8[26:31]
data: {
  "id": "chatcmpl-xxxx",
  "object": "chat.completion.chunk",
  "created": 1688198627,
  "model": "gpt-3.5-turbo-0613",
  "choices":[{
    "index": 0,
    "delta": {"content": " How"},
    "finish_reason": null
  }]
}
```

---

```js focus=8 mark=8[26:31]
data: {
  "id": "chatcmpl-xxxx",
  "object": "chat.completion.chunk",
  "created": 1688198627,
  "model": "gpt-3.5-turbo-0613",
  "choices":[{
    "index": 0,
    "delta": {"content": " can"},
    "finish_reason": null
  }]
}
```

---

```js focus=8 mark=8[26:29]
data: {
  "id": "chatcmpl-xxxx",
  "object": "chat.completion.chunk",
  "created": 1688198627,
  "model": "gpt-3.5-turbo-0613",
  "choices":[{
    "index": 0,
    "delta": {"content": " I"},
    "finish_reason": null
  }]
}
```

---

```js focus=8 mark=8[26:34]
data: {
  "id": "chatcmpl-xxxx",
  "object": "chat.completion.chunk",
  "created": 1688198627,
  "model": "gpt-3.5-turbo-0613",
  "choices":[{
    "index": 0,
    "delta": {"content": " assist"},
    "finish_reason": null
  }]
}
```

---

```js focus=8 mark=8[26:31]
data: {
  "id": "chatcmpl-xxxx",
  "object": "chat.completion.chunk",
  "created": 1688198627,
  "model": "gpt-3.5-turbo-0613",
  "choices":[{
    "index": 0,
    "delta": {"content": " you"},
    "finish_reason": null
  }]
}
```

---

```js focus=8 mark=8[26:29]
data: {
  "id": "chatcmpl-xxxx",
  "object": "chat.completion.chunk",
  "created": 1688198627,
  "model": "gpt-3.5-turbo-0613",
  "choices":[{
    "index": 0,
    "delta": {"content": " ?"},
    "finish_reason": null
  }]
}
```

---

```js focus=8:9 mark=9[22:27]
data: {
  "id": "chatcmpl-xxxx",
  "object": "chat.completion.chunk",
  "created": 1688198627,
  "model": "gpt-3.5-turbo-0613",
  "choices":[{
    "index": 0,
    "delta": {},
    "finish_reason": "stop"
  }]
}
```

</CH.Slideshow>

After that you'll receive one last chunk with the string _`"data: [DONE]"`_.

One thing we lose with streaming is the [`usage`](#usage) field. So if you need to know how many tokens the request used you'll need to count them yourself. It's a shame that OpenAI doesn't include it in the last chunk.

---

## Sampling

<nav id="sampling" />

```js focus=17:18
// from ../snippets/base.js
```

To control the randomness of the model's output you can use either the _`temperature`_ or _`top_p`_ parameters.

_`temperature`_ accept values between `0` and `2`. The default is `1`, higher values means more random answers. Lower values means more determinism. Use `0` if you want always the same answers for the same input, but be aware that there's still chance for randomness.

_`top_p`_ accept values between `0` and `1`. Same as before, lower values means less variation. The default is `1`.

OpenAI says it's not recommended to use both parameters at the same time. No idea why.

---

## Choices

<nav id="choices" />

```js focus=14,19:30 mark=14[3:6]
// from ../snippets/choices.js
```

You can use the _`n`_ parameter to get more than one response.

Each response will be a different object inside the _`choices`_ array.

Note that the content of each choice may be the same, specially for short answer or if you're using a low _`temperature`_.

The _`index`_ field may seem useless, but it's actually useful when using _`n`_ together with streaming. Each chunk will include content for one of the choices, and the _`index`_ field will tell you which one.

---

## Stop

<nav id="stop" />

```js focus=10,11,13,22:28 mark=13[10:13]
// from ../snippets/stop.js
```

_`stop`_ is a list of strings (case-sensitive) that will tell the model to stop generating text when it finds one of them. You can provide up to 4 stop sequences.

The stop sequences aren't included in the response.

Providing a _`stop`_ list won't make the answer longer, the model will stop naturally if it doesn't find any of the stop sequences.

---

## Max Tokens

<nav id="max-tokens" />

```js focus=10,11,13,21:25,30 mark=13[3:15],25[22:29]
// from ../snippets/maxtokens.js
```

Another way to limit the length of the response is to use the _`max_tokens`_ parameter.

The model will start generating text as usual, but it will count the output tokens and stop when it reaches the given limit. Only output tokens are counted towards the limit.

In the example, the five tokens are:

```json
["1", ".", " Lionel", " Messi", "\n"]
```

You can use the _`finish_reason`_ field to know if the model stopped naturally (_`finish_reason:`_ _`"stop"`_) or because of the token limit (_`finish_reason:`_ _`"length"`_).

---

## Penalties

<nav id="penalties" />

To avoid the model from repeating itself too much you can use the _`presence_penalty`_ and _`frequency_penalty`_ parameters.

Both are numbers between _`-2.0`_ and _`2.0`_ and default to _`0`_.

```js focus=22:23
// from ../snippets/base.js
```

A positive _`presence_penalty`_ makes it less likely for the model to repeat a token that has already been generated. The higher the penalty, the less likely a token will be repeated.

_`frequency_penalty`_ is similar, but the penalty increase every time the token is generated.

If I had to choose, I'd pick _`presence_penalty`_. _`frequency_penalty`_ might penalize too much tokens that repeat often, such as punctuation and articles.

---

## Logit Bias

<nav id="logit-bias" />

You can penalize or encourage specific tokens by using the _`logit_bias`_ parameter.

```js focus=24
// from ../snippets/base.js
```

_`logit_bias`_ is an object where the keys are token ids and values are the bias. The bias is a number between _`-100`_ and _`100`_. Like this:

```js
logit_bias: {
  65515: -5,
  71105: 10
}
```

A bias closer to _`-100`_ will make the model avoid the token, while a value closer to _`100`_ will make the model more likely to use the token (well, if you use _`100`_ the model will probably repeat that token until the token limit is reached).

Since the keys are ids, you'll need a [tool to get the token id](https://tiktokenizer.vercel.app/) from a string.

---

## Response

<nav id="response" />

```js focus=29:32
// from ../snippets/base.js
```

In the response, in addition to the _`choices`_ array, you'll get some extra fields:

- _`id`_: The unique id OpenAI gives to the response.
- _`object`_: Always _`"chat.completion"`_.
- _`created`_: The timestamp when the response was created. In case of streaming, this will stay the same for all chunks.
- _`model`_: The model used to generate the response. This is the full name of the model including the snapshot. For example if you used _`"gpt-3.5-turbo"`_ in your request, the response will be _`model: "gpt-3.5-turbo-0613"`_.

---

## Finish Reason

<nav id="finish-reason" />

```js focus=33,40,42
// from ../snippets/base.js
```

Every response will include a _`finish_reason`_. This field will tell you why the model stopped generating text. The possible reasons are:

- _`"stop"`_: The model returned a complete response, or it was stopped by a [`stop`](#stop) sequence.
- _`"length"`_: Reached the token limit set by [`max_tokens`](#max-tokens) or by the context limit of the model.
- _`"function_call"`_: The model decided to call a function.
- _`null`_: The response is still in progress.

---

## Usage

<nav id="usage" />

```js focus=43:47
// from ../snippets/base.js
```

The _`usage`_ field has information about the number of tokens used by the request (_`prompt_tokens`_) and the response (_`completion_tokens`_). This is important because those are the two numbers that will be used to calculate the cost of the request.

_`prompt_tokens`_ includes the tokens from all the content passed in the _`messages`_ and _`functions`_ fields.

_`completion_tokens`_ includes the tokens from all the _`choices`_ in the response.

Current [prices](https://openai.com/pricing) per 1,000 tokens:

| Model             | Prompt  | Completion |
| :---------------- | :------ | :--------- |
| gpt-3.5-turbo     | $0.0015 | $0.002     |
| gpt-3.5-turbo-16k | $0.003  | $0.004     |
| gpt-4             | $0.03   | $0.06      |
| gpt-4-32k         | $0.06   | $0.12      |

Let's say we have this usage in a GPT-4 request:

```json
{
  "model": "gpt-4-0613",
  "usage": {
    "prompt_tokens": 100,
    "completion_tokens": 150
  }
}
```

The cost of the request is $0.012:

```js
// prompt cost:
100 * 0.03 / 1000 +
// completion cost:
150 * 0.06 / 1000
// total cost:
= 0.012
```

---

## Errors

<nav id="usage" />

`// TODO`

https://platform.openai.com/docs/guides/error-codes/api-errors

</CH.Scrollycoding>
